Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Policy/Curiosity Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Policy/Curiosity Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Losses/Curiosity Forward Loss,Losses/Curiosity Inverse Loss,Is Training
10000,1.418709,81.43333333333334,0.026775919,0.13482422,-3.239499827971061,-3.239499827971061,0.8366569300492604,0.71513635,0.18644772,0.0002995619,0.19985393,0.004992713,4.758371,0.5489051,1.0
20000,1.418438,66.6,-0.19757874,0.41336426,-3.109940052432502,-3.109940052432502,2.0514883224996145,0.7746493,0.14295544,0.00029912262,0.19970752,0.0049854065,1.4428307,0.45293543,1.0
30000,1.4186538,89.13513513513513,-0.505849,1.0074866,-3.158478841051325,-3.158478841051325,1.5184951937413431,0.56850344,0.13716644,0.00029847055,0.19949019,0.00497456,0.82978153,0.43899947,1.0
40000,1.41864,93.16190476190476,-0.78764105,1.205798,-3.0750201670628674,-3.0750201670628674,1.0914122425722625,0.43743315,0.15365782,0.0002977881,0.19926268,0.0049632094,0.56735474,0.42936224,1.0
50000,1.4186524,108.40217391304348,-1.0246177,1.2446371,-2.6464683086327883,-2.6464683086327883,0.8887407466283311,0.32136342,0.1480033,0.00029732083,0.19910693,0.0049554356,0.41769925,0.43087822,1.0
60000,1.4181815,161.6451612903226,-1.1412594,1.1661043,-1.4046974533894023,-1.4046974533894023,0.9966000518532562,0.20069358,0.05962762,0.00029678346,0.19892782,0.0049464977,0.3265444,0.42665574,1.0
70000,1.4177288,215.55555555555554,-1.0641694,1.0843455,-0.05561095022636911,-0.05561095022636911,1.0830322970514712,0.09888824,0.05220544,0.00029620758,0.1987359,0.004936919,0.26122501,0.42207342,1.0
80000,1.4171313,218.10638297872342,-0.8793045,0.9652788,0.07720321773186974,0.07720321773186974,0.921103105596874,0.05862063,0.12297947,0.0002956206,0.19854021,0.004927156,0.21789794,0.42291528,1.0
