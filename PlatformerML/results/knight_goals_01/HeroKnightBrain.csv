Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Policy/Curiosity Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Policy/Curiosity Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Losses/Curiosity Forward Loss,Losses/Curiosity Inverse Loss,Is Training
40000,1.4162003,16.088028169014084,0.7812912,0.768416,0.8555109045952032,0.8555109045952032,0.041574818730459616,0.20601024,0.20794754,0.0002977229,0.19924094,0.004962124,0.30763626,0.41652653,1.0
50000,1.4149442,15.616279069767442,0.7783737,0.68228847,0.8392557650309465,0.8392557650309465,0.07231045753209298,0.19240181,0.26764828,0.0002973263,0.19910878,0.0049555274,0.22411157,0.42190883,1.0
60000,1.4133301,16.606326889279437,0.7481036,0.538872,0.8084823447214046,0.8084823447214046,0.04939223689348503,0.22207709,0.24469519,0.00029666137,0.19888714,0.004944467,0.1384677,0.4165727,1.0
70000,1.4113296,17.99809885931559,0.7062913,0.38782397,0.8523786988611239,0.8523786988611239,0.042516620714832414,0.18808168,0.21348189,0.00029604117,0.19868039,0.0049341503,0.11063894,0.41738728,1.0
